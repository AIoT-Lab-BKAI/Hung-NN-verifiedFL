{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f90d349631414db46dea413ac2ff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dbd8b241e24e8e89c34818175be32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf87ce46a2ba4aa0b78e800fd58b0c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b5fece4c9546cd860389bc267bd879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")\n",
    "\n",
    "testing_data = datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each client:**\n",
    "\n",
    "1. Contains no more than 3 labels\n",
    "2. Each label has 8 to 20 samples\n",
    "3. There're at least 5 * #numclass clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = np.unique(training_data.targets).tolist()\n",
    "len(total_labels)\n",
    "\n",
    "min_label_per_client = 2\n",
    "max_label_per_client = 3\n",
    "\n",
    "min_sample_per_client = 8\n",
    "max_sample_per_client = 20\n",
    "\n",
    "# num_clients = 5 * len(total_labels)\n",
    "num_clients = 2\n",
    "\n",
    "total_label = len(total_labels)\n",
    "label_list = [i for i in range(total_label)]\n",
    "label_per_client = 2\n",
    "\n",
    "labels = training_data.targets\n",
    "idxs = range(len(training_data))\n",
    "training_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "labels = testing_data.targets\n",
    "idxs = range(len(testing_data))\n",
    "testing_idxs_labels = np.vstack((idxs, labels)).T\n",
    "\n",
    "training_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "testing_dict_client = {client_id:[] for client_id in range(num_clients)}\n",
    "\n",
    "# client_labels = []\n",
    "client_labels = [[0, 1], [2, 3]]\n",
    "# not_passed_label_list = label_list.copy()\n",
    "\n",
    "# for client_id in range(num_clients):\n",
    "#     label_per_client = np.random.randint(min_label_per_client, max_label_per_client + 1)\n",
    "#     this_set = np.random.choice(label_list, label_per_client, replace=False)\n",
    "#     client_labels.append(list(this_set))\n",
    "#     not_passed_label_list = list(set(not_passed_label_list) - set(this_set))\n",
    "\n",
    "# if len(not_passed_label_list) > 0:\n",
    "#     print(\"Uncover\", len(not_passed_label_list), \"labels !\")\n",
    "#     exit(0)\n",
    "# else:\n",
    "#     print(\"Uncover\", len(not_passed_label_list), \"labels !\")\n",
    "\n",
    "for client_idx, client_label in zip(range(num_clients), client_labels):\n",
    "    sample_per_client = np.random.randint(min_sample_per_client, max_sample_per_client + 1)\n",
    "    \n",
    "    for label in client_label:\n",
    "        idxes_1 = training_idxs_labels[training_idxs_labels[:,1] == label][:,0]\n",
    "        idxes_2 = testing_idxs_labels[testing_idxs_labels[:,1] == label][:,0]\n",
    "        \n",
    "        label_1_idxes = np.random.choice(idxes_1, sample_per_client, replace=False)\n",
    "        label_2_idxes = np.random.choice(idxes_2, int(sample_per_client/4), replace=False)\n",
    "        \n",
    "        training_dict_client[client_idx] += label_1_idxes.tolist()\n",
    "        testing_dict_client[client_idx] += label_2_idxes.tolist()\n",
    "        \n",
    "        training_idxs_labels[label_1_idxes] -= 100\n",
    "        testing_idxs_labels[label_2_idxes] -= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "savepath = \"./jsons/baseline/simple_6\"\n",
    "if not Path(savepath).exists():\n",
    "    os.makedirs(savepath)\n",
    "    \n",
    "json.dump(training_dict_client, open(f\"{savepath}/clients_training_data.json\", \"w\"), cls=NumpyEncoder)\n",
    "json.dump(testing_dict_client, open(f\"{savepath}/clients_testing_data.json\", \"w\"), cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('longnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
