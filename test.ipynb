{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from utils.fmodule import FModule, get_module_from_model\n",
    "import utils.fmodule as fmodule\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "class DNN_proposal(FModule):\n",
    "    def __init__(self, input_dim = 784, mid_dim = 100, output_dim = 10):\n",
    "        super().__init__()\n",
    "        # define network layers\n",
    "        self.fc1 = nn.Linear(input_dim, mid_dim)\n",
    "        self.fc2 = nn.Linear(mid_dim, output_dim)\n",
    "        # mask regenerator\n",
    "        self.mg_fc1 = nn.Linear(mid_dim, 128)\n",
    "        self.mg_fc2 = nn.Linear(128, output_dim)\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def  __call__(self, x, original_mask_diagonal=None):            \n",
    "        return self.forward(x, original_mask_diagonal)\n",
    "    \n",
    "    def forward(self, x, original_mask_diagonal=None):\n",
    "        r_x = self.encoder(x).detach()\n",
    "        l_x = self.decoder(r_x)\n",
    "        dm_x = self.mask_diagonal_regenerator(r_x).detach()\n",
    "        dm_x = (dm_x > 1/10) * 1.\n",
    "        m_x = torch.diag_embed(dm_x)\n",
    "        \n",
    "        if original_mask_diagonal is None:\n",
    "            \"\"\" When inference \"\"\"\n",
    "            suro_l_x = l_x\n",
    "        else:\n",
    "            \"\"\" When training \"\"\"\n",
    "            suro_l_x = self.surogate_logits(l_x, torch.diag_embed(original_mask_diagonal))\n",
    "        \n",
    "        mirr_suro_l_x = self.mirror_surogate_logits(suro_l_x, m_x)\n",
    "        # output = F.log_softmax(mirr_suro_l_x, dim=1)\n",
    "        output = mirr_suro_l_x\n",
    "        return output\n",
    "    \n",
    "    def mask_diagonal(self, x):\n",
    "        \"\"\"\n",
    "        This function returns the mask's diagonal vector of x\n",
    "        \"\"\"\n",
    "        r_x = self.encoder(x).detach()\n",
    "        dm_x = self.mask_diagonal_regenerator(r_x)\n",
    "        return dm_x\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        \"\"\"\n",
    "        This function returns the representation of x\n",
    "        \"\"\"\n",
    "        r_x = torch.flatten(x, 1)\n",
    "        r_x = torch.sigmoid(self.fc1(r_x))\n",
    "        return r_x\n",
    "    \n",
    "    def decoder(self, r_x):\n",
    "        \"\"\"\n",
    "        This function returns the logits of r_x\n",
    "        \"\"\"\n",
    "        l_x = self.fc2(r_x)\n",
    "        return l_x\n",
    "    \n",
    "    def mask_diagonal_regenerator(self, r_x):\n",
    "        \"\"\"\n",
    "        This function generate a mask's diagonal vector for each element in r_x,\n",
    "        returning shape of b x 10\n",
    "        \"\"\"\n",
    "        dm_x = F.relu(self.mg_fc1(r_x))\n",
    "        dm_x = torch.softmax(self.mg_fc2(dm_x), dim=1)\n",
    "        dm_x = dm_x.view(r_x.shape[0], 10)\n",
    "        return dm_x\n",
    "           \n",
    "    def surogate_logits(self, l_x, original_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            l_x             : b x 10\n",
    "            original_mask   : 10 x 10\n",
    "        \n",
    "        This function return the logits that are masked,\n",
    "        the returning shape b x 10\n",
    "        \"\"\"\n",
    "        l_x = l_x.unsqueeze(2)\n",
    "        suro_l_x = (original_mask * 1.0) @ l_x\n",
    "        return suro_l_x.squeeze(2)\n",
    "    \n",
    "    def mirror_surogate_logits(self, suro_l_x, m_x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            suro_l_x: b x 10\n",
    "            m_x     : b x 10 x 10\n",
    "        \n",
    "        This function perform dot multiplication of m_x and suro_l_x,\n",
    "        returning the matrix of shape b x 10\n",
    "        \"\"\"\n",
    "        mirr_suro_l_x = m_x @ suro_l_x.unsqueeze(2)\n",
    "        return mirr_suro_l_x.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils.dataloader import CustomDataset\n",
    "import json\n",
    "from utils.train_smt import NumpyEncoder, batch_similarity, print_cfmtx, test\n",
    "import numpy as np\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "    \n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    ")\n",
    "\n",
    "client_id_list = [0,1,2,3,4]\n",
    "clients_dataset = [CustomDataset(training_data, json.load(open(f\"./jsons/client{client_id}.json\", 'r'))) for client_id in client_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_diagonal(dim, dataset):\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "    mask = torch.zeros([dim])\n",
    "    for X, y in train_dataloader:\n",
    "        label = y.item()\n",
    "        mask[label] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_training(dataloader, model, loss_fn, optimizer, device=\"cuda:1\"):\n",
    "    \"\"\"\n",
    "    This method trains for a discriminative representation space,\n",
    "    using constrastive learning\n",
    "    \n",
    "    Args:\n",
    "        dataloader: batch_size of 2, drop_last = True\n",
    "        loss_fn:    mean square error\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    same_class_dis = []\n",
    "    different_class_dis = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        representations = model.encoder(X)\n",
    "        \n",
    "        alpha = 1.0 if y[0].item() == y[1].item() else -1.0\n",
    "        distance = loss_fn(representations[0], representations[1])\n",
    "        loss = alpha * distance\n",
    "                \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if alpha > 0:\n",
    "            same_class_dis.append(distance.detach().item())\n",
    "        else:\n",
    "            different_class_dis.append(distance.detach().item())\n",
    "            \n",
    "    return np.mean(same_class_dis), np.mean(different_class_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 Same:  0.053941293309132256 Diff 0.0785806675752004\n",
      "Epochs 1 Same:  0.045783394326766334 Diff 0.10896965861320496\n",
      "Epochs 2 Same:  0.0423656408675015 Diff 0.22712791711091995\n",
      "Epochs 3 Same:  0.03296972004075845 Diff 0.21574803193410239\n",
      "Epochs 4 Same:  0.026166454330086707 Diff 0.268422394990921\n",
      "Epochs 5 Same:  0.004615426994860172 Diff 0.42212681770324706\n",
      "Epochs 6 Same:  0.005526202265173197 Diff 0.5099455356597901\n",
      "Epochs 7 Same:  0.005472602788358927 Diff 0.5869247317314148\n"
     ]
    }
   ],
   "source": [
    "client_id = 0\n",
    "mydataset = clients_dataset[client_id]\n",
    "clients_mask_diagonal = [None for client_id in client_id_list]\n",
    "\n",
    "if clients_mask_diagonal[client_id] is None:\n",
    "    clients_mask_diagonal[client_id] = create_mask_diagonal(dim=10, dataset=mydataset)\n",
    "    \n",
    "train_dataloader = DataLoader(mydataset, batch_size=2, shuffle=True, drop_last=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "model = DNN_proposal()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(8):\n",
    "    same, diff = representation_training(train_dataloader, model, loss_fn, optimizer, \"cuda\")\n",
    "    print(\"Epochs\", t, \"Same: \", same, \"Diff\", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_training(dataloader, model, optimizer, original_mask_diagonal, device=\"cuda:1\"):\n",
    "    \"\"\"\n",
    "    This method trains to make the model generate a mask that is\n",
    "    close to the original mask\n",
    "    \"\"\"\n",
    "    original_mask_diagonal = original_mask_diagonal.to(device)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        mirr_mask_diagonal = model.mask_diagonal(X)\n",
    "        mask_loss = torch.sum(torch.pow(mirr_mask_diagonal - original_mask_diagonal, 2))/mirr_mask_diagonal.shape[0]\n",
    "        loss = mask_loss\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mirr_mask_diagonal = (mirr_mask_diagonal > 1/10) * 1.0\n",
    "        loss = torch.sum(torch.abs(mirr_mask_diagonal - original_mask_diagonal))/mirr_mask_diagonal.shape[0]\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    # print(\"Masking losses\", losses)\n",
    "    return np.mean(losses), mirr_mask_diagonal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mask diag [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Epochs 0 mask_loss:  4.8125\n",
      "Epochs 1 mask_loss:  2.0\n",
      "Epochs 2 mask_loss:  0.3125\n",
      "Epochs 3 mask_loss:  0.5\n",
      "Epochs 4 mask_loss:  0.0625\n",
      "Epochs 5 mask_loss:  0.0\n",
      "Epochs 6 mask_loss:  0.0\n",
      "Epochs 7 mask_loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "model2 = copy.deepcopy(model)\n",
    "train_dataloader = DataLoader(mydataset, batch_size=4, shuffle=True, drop_last=False)\n",
    "mask_optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "# Train the mask first\n",
    "epoch_loss = []\n",
    "print(\"True mask diag\", clients_mask_diagonal[client_id].tolist())\n",
    "for t in range(8):\n",
    "    mask_loss, mirr_mask_diagonal = mask_training(train_dataloader, model2, mask_optimizer, clients_mask_diagonal[client_id], \"cuda\")\n",
    "    print(\"Epochs\", t, \"mask_loss: \", mask_loss)\n",
    "    # print(\"Masking loss\", loss.detach().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_training(dataloader, model, loss_fn, optimizer, original_mask, device=\"cuda:1\"):\n",
    "    original_mask = original_mask.to(device)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    losses = []\n",
    "        \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X, original_mask)\n",
    "        classification_loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        classification_loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(classification_loss.item())\n",
    "    \n",
    "    # print(\"classification losses\", losses)\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 loss:  2.008186399936676\n",
      "Epochs 1 loss:  1.8600953221321106\n",
      "Epochs 2 loss:  1.715594321489334\n",
      "Epochs 3 loss:  1.6210212111473083\n",
      "Epochs 4 loss:  1.4336721301078796\n",
      "Epochs 5 loss:  1.3050436675548553\n",
      "Epochs 6 loss:  1.2295474410057068\n",
      "Epochs 7 loss:  1.1053496301174164\n"
     ]
    }
   ],
   "source": [
    "model3 = copy.deepcopy(model2)\n",
    "train_dataloader = DataLoader(mydataset, batch_size=4, shuffle=True, drop_last=False)\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Train the mask first\n",
    "epoch_loss = []\n",
    "for t in range(8):\n",
    "    loss = classification_training(train_dataloader, model3, loss_fn, optimizer, clients_mask_diagonal[client_id], \"cuda\")\n",
    "    print(\"Epochs\", t, \"loss: \", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('longnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
