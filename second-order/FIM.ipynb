{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _check_param_device(param, old_param_device):\n",
    "    r\"\"\"This helper function is to check if the parameters are located\n",
    "    in the same device. Currently, the conversion between model parameters\n",
    "    and single vector form is not supported for multiple allocations,\n",
    "    e.g. parameters in different GPUs, or mixture of CPU/GPU.\n",
    "\n",
    "    Arguments:\n",
    "        param ([Tensor]): a Tensor of a parameter of a model\n",
    "        old_param_device (int): the device where the first parameter of a\n",
    "                                model is allocated.\n",
    "\n",
    "    Returns:\n",
    "        old_param_device (int): report device for the first time\n",
    "    \"\"\"\n",
    "\n",
    "    # Meet the first parameter\n",
    "    if old_param_device is None:\n",
    "        old_param_device = param.get_device() if param.is_cuda else -1\n",
    "    else:\n",
    "        warn = False\n",
    "        if param.is_cuda:  # Check if in same GPU\n",
    "            warn = (param.get_device() != old_param_device)\n",
    "        else:  # Check if in CPU\n",
    "            warn = (old_param_device != -1)\n",
    "        if warn:\n",
    "            raise TypeError('Found two parameters on different devices, '\n",
    "                            'this is currently not supported.')\n",
    "    return old_param_device\n",
    "\n",
    "\n",
    "def vector_to_parameter_list(vec, parameters):\n",
    "    r\"\"\"Convert one vector to the parameter list\n",
    "\n",
    "    Arguments:\n",
    "        vec (Tensor): a single vector represents the parameters of a model.\n",
    "        parameters (Iterable[Tensor]): an iterator of Tensors that are the\n",
    "            parameters of a model.\n",
    "    \"\"\"\n",
    "    # Ensure vec of type Tensor\n",
    "    if not isinstance(vec, torch.Tensor):\n",
    "        raise TypeError('expected torch.Tensor, but got: {}'\n",
    "                        .format(torch.typename(vec)))\n",
    "    # Flag for the device where the parameter is located\n",
    "    param_device = None\n",
    "    params_new = []\n",
    "    # Pointer for slicing the vector for each parameter\n",
    "    pointer = 0\n",
    "    for param in parameters:\n",
    "        # Ensure the parameters are located in the same device\n",
    "        param_device = _check_param_device(param, param_device)\n",
    "\n",
    "        # The length of the parameter\n",
    "        num_param = param.numel()\n",
    "        # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "        param_new = vec[pointer:pointer + num_param].view_as(param).data\n",
    "        params_new.append(param_new)\n",
    "        # Increment the pointer\n",
    "        pointer += num_param\n",
    "\n",
    "    return list(params_new)\n",
    "\n",
    "\n",
    "def Rop(ys, xs, vs):\n",
    "    if isinstance(ys, tuple):\n",
    "        ws = [torch.tensor(torch.zeros_like(y), requires_grad=True) for y in ys]\n",
    "    else:\n",
    "        ws = torch.tensor(torch.zeros_like(ys), requires_grad=True)\n",
    "\n",
    "    gs = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\n",
    "    re = torch.autograd.grad(gs, ws, grad_outputs=vs, create_graph=True, retain_graph=True, allow_unused=True)\n",
    "    return tuple([j.detach() for j in re])\n",
    "\n",
    "\n",
    "def Lop(ys, xs, ws):\n",
    "    vJ = torch.autograd.grad(ys, xs, grad_outputs=ws, create_graph=True, retain_graph=True, allow_unused=True)\n",
    "    return tuple([j.detach() for j in vJ])\n",
    "\n",
    "\n",
    "def HesssianVectorProduct(f, x, v):\n",
    "    df_dx = torch.autograd.grad(f, x, create_graph=True, retain_graph=True)\n",
    "    Hv = Rop(df_dx, x, v)\n",
    "    return tuple([j.detach() for j in Hv])\n",
    "\n",
    "\n",
    "def FisherVectorProduct(loss, output, model, vp):\n",
    "    Jv = Rop(output, list(model.parameters()), vp)\n",
    "    batch, dims = output.size(0), output.size(1)\n",
    "    if loss.grad_fn.__class__.__name__ == 'NllLossBackward':\n",
    "        outputsoftmax = torch.nn.functional.softmax(output, dim=1)\n",
    "        M = torch.zeros(batch, dims, dims).cuda() if outputsoftmax.is_cuda else torch.zeros(batch, dims, dims)\n",
    "        M.reshape(batch, -1)[:, ::dims + 1] = outputsoftmax\n",
    "        H = M - torch.einsum('bi,bj->bij', (outputsoftmax, outputsoftmax))\n",
    "        HJv = [torch.squeeze(H @ torch.unsqueeze(Jv[0],-1)) / batch] \n",
    "    else:\n",
    "        HJv = HesssianVectorProduct(loss, output, Jv)\n",
    "    JHJv = Lop(output, list(model.parameters()), HJv)\n",
    "    return torch.cat([torch.flatten(v) for v in JHJv])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:57:06) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f784b053654bb8129a3cb1aa1762d7834caeb9ba8691a85058f59d7796858ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
